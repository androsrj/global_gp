outcomes.df$Prob <- as.numeric(unname(props))
outcomes.df <- outcomes.df %>% arrange(Prob)
head(outcomes.df)
tail(outcomes.df)
evs <- c(11, 16, 15, 6, 16, 19, 10)
trump.start <- 219
harris.start <- 226
outcomes.df$Winner <- numeric(nrow(outcomes.df))
for (i in 1:nrow(outcomes.df)) {
trump.ev <- trump.start + sum(evs * (outcomes.df[i, 1:7] == "Trump"))
harris.ev <- harris.start + sum(evs * (outcomes.df[i, 1:7] == "Harris"))
if (trump.ev >= 270) {
outcomes.df$Winner[i] <- "Trump"
} else if (harris.ev >= 270) {
outcomes.df$Winner[i] <- "Harris"
} else {
outcomes.df$Winner[i] <- "Tie"
}
}
table(outcomes.df$Winner)
aggregate(data = outcomes.df, Prob ~ Winner, sum)
tail(outcomes.df)
library(mvtnorm)
library(stringr)
library(dplyr)
use.2024 <- TRUE
margins <- data.frame(
az = c(8.5, 10.1, 3.5, -0.4, 1.7),
ga = c(5.2, 8.0, 5.1, -0.3, 1.2),
mi = c(-16.8, -9.5, 0.3, -2.8, -1.8),
nv = c(-12.5, -6.6, -2.4, -2.4, 0.6),
nc = c(-0.3, 2.2, 3.6, 1.3, 1.3),
pa = c(-10.3, -5.2, 0.7, -1.2, 0.0),
wi = c(-13.9, -6.7, 0.7, -0.6, -1.1)
)
if (use.2024 == FALSE) {
margins <- margins[-5, ]
rownames(margins) <- seq(2008, 2020, by = 4)
} else {
rownames(margins) <- seq(2008, 2024, by = 4)
}
round(cor(margins), 2)
round(cov(margins), 2)
years <- c(2008, 2012, 2016, 2020, 2024)
weights <- c(1, 1, 3, 5, 10)
weighted.mean <- function(x, w) sum(x * w) / sum(w)
means <- apply(margins, 2, weighted.mean, w = weights)
X.centered <- as.matrix(sweep(margins, 2, means, "-"))
cov.weighted <- (t(X.centered) %*% (X.centered * weights)) / sum(weights)
#Sigma <- cov(margins)
Sigma <- cov.weighted
mu <- rep(0, 7)
nReps <- 1000000
sim <- rmvnorm(n = nReps, mean = mu, sigma = Sigma)
wins <- sim > 0
outcomes <- apply(wins, 1, paste0, collapse = "")
props <- table(outcomes) / nReps
combos <- names(props)
length(props)
sort(props)
states <- c("Arizona", "Georgia", "Michigan", "Nevada", "North Carolina", "Pennsylvania", "Wisconsin")
outcomes.df <- t(sapply(1:length(props), function(i) {
as.logical(str_extract_all(combos[i], "TRUE|FALSE")[[1]])
}))
dim(outcomes.df)
colnames(outcomes.df) <- states
outcomes.df[outcomes.df == TRUE] <- "Trump"
outcomes.df[outcomes.df == FALSE] <- "Harris"
outcomes.df <- as.data.frame(outcomes.df)
outcomes.df$Prob <- as.numeric(unname(props))
outcomes.df <- outcomes.df %>% arrange(Prob)
head(outcomes.df)
tail(outcomes.df)
evs <- c(11, 16, 15, 6, 16, 19, 10)
trump.start <- 219
harris.start <- 226
outcomes.df$Winner <- numeric(nrow(outcomes.df))
for (i in 1:nrow(outcomes.df)) {
trump.ev <- trump.start + sum(evs * (outcomes.df[i, 1:7] == "Trump"))
harris.ev <- harris.start + sum(evs * (outcomes.df[i, 1:7] == "Harris"))
if (trump.ev >= 270) {
outcomes.df$Winner[i] <- "Trump"
} else if (harris.ev >= 270) {
outcomes.df$Winner[i] <- "Harris"
} else {
outcomes.df$Winner[i] <- "Tie"
}
}
table(outcomes.df$Winner)
aggregate(data = outcomes.df, Prob ~ Winner, sum)
tail(outcomes.df)
# Shrink the covariance matrix towards the identity
cov.matrix <- cov(margins)
lambda <- 0.2
target <- matrix(0.3, 7, 7)
diag(target) <- 1
shrunk.cov <- lambda * target + (1 - lambda) * cov.matrix
shrunk.cov
cor.matrix <- cor(margins)
target <- matrix(0.3, 7, 7)
diag(target) <- 1
shrunk.cor <- lambda * target + (1 - lambda) * cor.matrix
shrunk.cor
round(cor(margins), 2)
lambda <- 0.5
cor.matrix <- cor(margins)
target <- matrix(0.5, 7, 7)
diag(target) <- 1
shrunk.cor <- lambda * target + (1 - lambda) * cor.matrix
shrunk.cor
Sigma <- cov(margins)
Sigma[Sigma < 0] <- 0.1
mu <- rep(0, 7)
nReps <- 1000000
sim <- rmvnorm(n = nReps, mean = mu, sigma = Sigma)
wins <- sim > 0
outcomes <- apply(wins, 1, paste0, collapse = "")
props <- table(outcomes) / nReps
combos <- names(props)
length(props)
Sigma <- cov(margins)
Sigma[Sigma < 0] <- 0
mu <- rep(0, 7)
nReps <- 1000000
sim <- rmvnorm(n = nReps, mean = mu, sigma = Sigma)
data()
head(AirPassengers)
AirPassengers
class(AirPassengers)
BOD
lynx
mean(lynx)
hist(lynx)
sd(lynx)
head(airquality)
setwd("~/research/global_gp")
library(spBayes)
source("other_functions/helper_functions.R")
# Scenario 1
train <- readRDS("data/small/scen1/train.RDS")
test <- readRDS("data/small/scen1/test.RDS")
scen=1
d.max <- max(iDist(train$U))
r <- 3
n <- nrow(train$X)
nTest <- nrow(test$X)
priors <- list("phi.Unif"=list(rep(3/(0.75*d.max), r), rep(3/(0.001*d.max), r)),
"sigma.sq.IG"=list(rep(2, r), rep(1, r)),
"tau.sq.IG"=c(2, 1))
starting <- list("phi"=rep(3/(0.1*d.max), r), "sigma.sq"=rep(1, r), "tau.sq"=1)
tuning <- list("phi"=rep(0.1, r), "sigma.sq"=rep(0.05, r), "tau.sq"=0.1)
n.samples <- 5000
# Train original model
m.1 <- spSVC(train$Y[1:nrow(train$X),] ~ train$X, coords = train$U,
starting = starting, svc.cols = 1:3,
tuning = tuning, priors = priors, cov.model = "exponential",
n.samples = n.samples, n.report = 5000, n.omp.threads = 4)
# Recover SVC coefficients (training data)
m.2 <- spRecover(m.1, start = floor(0.5*n.samples), thin=2,
n.omp.threads = 4, verbose = FALSE)
# Estimate SVC coefficients (testing data)
m.3 <- spPredict(m.2, pred.coords = test$U + runif(50, -1e6, 1e6),
pred.covars = cbind(rep(1, nTest), test$X))
dim(m.2$p.w.recover.samples)
dim(m.2$p.beta.recover.samples)
beta <- apply(m.2$p.beta.recover.samples, 2, mean)
beta
w <_ apply(m.2$p.w.recover.samples, 1, mean)
w <- apply(m.2$p.w.recover.samples, 1, mean)
dim(w)
length(w)
beta0 <- beta[1] + w[1:100]
mean(beta0)
hist(beta0)
dim(train$U)
plt.data <- data.frame(train$U, beta0)
head(plt.data)
library(ggplot2)
ggplot(data = plt.data, aes(x = X1, y = X2, col = beta0)) + geom_point()
ggplot(data = plt.data, aes(x = X1, y = X2, col = beta0)) + geom_point(size=5)
mba.interp <- mba.surf(plt.data, no.X=100, no.Y=100, extend=TRUE)
library(MBA)
mba.interp <- mba.surf(plt.data, no.X=100, no.Y=100, extend=TRUE)
image(mba.interp$xyz.est, main = "",
col = tim.colors(64), cex.main = 1.5)
library(fields)
mba.interp <- mba.surf(plt.data, no.X=100, no.Y=100, extend=TRUE)
image(mba.interp$xyz.est, main = "",
col = tim.colors(64), cex.main = 1.5)
## Not run:
rmvn <- function(n, mu=0, V = matrix(1)){
p <- length(mu)
if(any(is.na(match(dim(V),p))))
stop("Dimension problem!")
D <- chol(V)
t(matrix(rnorm(n*p), ncol=p)%*%D + rep(mu,rep(n,p)))
}
set.seed(1)
n <- 200
coords <- cbind(runif(n,0,1), runif(n,0,1))
X <- as.matrix(cbind(1, rnorm(n)))
B <- as.matrix(c(1,5))
p <- length(B)
sigma.sq <- 10
tau.sq <- 0.01
phi <- 3/0.5
D <- as.matrix(dist(coords))
R <- exp(-phi*D)
w <- rmvn(1, rep(0,n), sigma.sq*R)
y <- rnorm(n, X%*%B + w, sqrt(tau.sq))
##partition the data for out of sample prediction
mod <- 1:100
y.mod <- y[mod]
X.mod <- X[mod,]
coords.mod <- coords[mod,]
n.samples <- 1000
starting <- list("phi"=3/0.5, "sigma.sq"=50, "tau.sq"=1)
tuning <- list("phi"=0.1, "sigma.sq"=0.1, "tau.sq"=0.1)
priors <- list("beta.Flat", "phi.Unif"=c(3/1, 3/0.1),
"sigma.sq.IG"=c(2, 5), "tau.sq.IG"=c(2, 0.01))
cov.model <- "exponential"
m.1 <- spLM(y.mod~X.mod-1, coords=coords.mod, starting=starting, tuning=tuning,
priors=priors, cov.model=cov.model, n.samples=n.samples)
m.1.pred <- spPredict(m.1, pred.covars=X, pred.coords=coords,
start=0.5*n.samples)
y.hat <- apply(m.1.pred$p.y.predictive.samples, 1, mean)
quant <- function(x){quantile(x, prob=c(0.025, 0.5, 0.975))}
y.hat <- apply(m.1.pred$p.y.predictive.samples, 1, quant)
plot(y, y.hat[2,], pch=19, cex=0.5, xlab="observed y", ylab="predicted y")
arrows(y[-mod], y.hat[2,-mod], y[-mod], y.hat[1,-mod], angle=90, length=0.05)
arrows(y[-mod], y.hat[2,-mod], y[-mod], y.hat[3,-mod], angle=90, length=0.05)
## End(Not run)
B
w
dim(w)
dim(X)
dim(m.1.pred$p.y.predictive.samples)
library(spBayes)
source("other_functions/helper_functions.R")
# Scenario 1
train <- readRDS("data/small/scen1/train.RDS")
test <- readRDS("data/small/scen1/test.RDS")
scen=1
d.max <- max(iDist(train$U))
r <- 3
n <- nrow(train$X)
nTest <- nrow(test$X)
priors <- list("phi.Unif"=list(rep(3/(0.75*d.max), r), rep(3/(0.001*d.max), r)),
"sigma.sq.IG"=list(rep(2, r), rep(1, r)),
"tau.sq.IG"=c(2, 1))
starting <- list("phi"=rep(3/(0.1*d.max), r), "sigma.sq"=rep(1, r), "tau.sq"=1)
tuning <- list("phi"=rep(0.1, r), "sigma.sq"=rep(0.05, r), "tau.sq"=0.1)
n.samples <- 5000
# Train original model
m.1 <- spSVC(train$Y[1:nrow(train$X),] ~ train$X, coords = train$U,
starting = starting, svc.cols = 1:3,
tuning = tuning, priors = priors, cov.model = "exponential",
n.samples = n.samples, n.report = 5000, n.omp.threads = 4)
# Recover SVC coefficients (training data)
m.2 <- spRecover(m.1, start = floor(0.5*n.samples), thin=2,
n.omp.threads = 4, verbose = FALSE)
head(test$U)
head(test$U + runif(50, -1e6, 1e6))
# Estimate SVC coefficients (testing data)
m.3 <- spPredict(m.2, pred.coords = test$U + runif(50, -1e-6, 1e-6),
pred.covars = cbind(rep(1, nTest), test$X))
m.3 <- spPredict(m.2, pred.coords = test$U,
pred.covars = cbind(rep(1, nTest), test$X))
?sample
?sample
for (i in 1:10) {
print(i)
}
for (i in 1:10) {
print(i^2)
}
for (i in 1:10) {
x = 3*i^2 - 2*i + 9
print(x)
}
for (i in 1:10) {
#x = 3*i^2 - 2*i + 9
#print(x)
print("Hello")
}
rep("ABC", 6)
rep(0, 6)
rep(NA, 6)
number.heads = rep(NA, nSims)
nSims = 10000
coin = c("H", "T")
number.heads = rep(NA, nSims)
number.heads
nSims = 6
coin = c("H", "T")
number.heads = rep(NA, nSims)
number.heads
i=1
my.data = sample(coin, size = 10, replace = TRUE, prob = c(0.75, 0.25))
my.data
number.heads
#my.data = sample(coin, 10, TRUE)
number.heads[i] = sum(my.data == "H")
number.heads
i=2
my.data = sample(coin, size = 10, replace = TRUE, prob = c(0.75, 0.25))
my.data
#my.data = sample(coin, 10, TRUE)
number.heads[i] = sum(my.data == "H")
number.heads
for (i in 1:nSims) {
my.data = sample(coin, size = 10, replace = TRUE, prob = c(0.75, 0.25))
#my.data = sample(coin, 10, TRUE)
number.heads[i] = sum(my.data == "H")
}
number.heads
nSims = 10000
coin = c("H", "T")
number.heads = rep(NA, nSims)
for (i in 1:nSims) {
my.data = sample(coin, size = 10, replace = TRUE, prob = c(0.75, 0.25))
#my.data = sample(coin, 10, TRUE)
number.heads[i] = sum(my.data == "H")
}
number.heads
mean(number.heads)
nSims = 7
coin = c("H", "T")
number.heads = rep(NA, nSims)
number.heads
rep("qwerty", 5)
rep(0, 5)
rep(NA, 5)
nSims = 7
coin = c("H", "T")
number.heads = rep(NA, nSims)
nSims = 7
coin = c("H", "T")
number.heads = rep(NA, nSims)
i=1
my.data = sample(coin, size = 10, replace = TRUE, prob = c(0.75, 0.25))
my.data
sum(my.data == "H")
number.heads[i] = sum(my.data == "H")
number.heads
i=2
my.data = sample(coin, size = 10, replace = TRUE, prob = c(0.75, 0.25))
my.data
sum(my.data == "H")
number.heads[i] = sum(my.data == "H")
number.heads
nSims = 7
coin = c("H", "T")
number.heads = rep(NA, nSims)
for (i in 1:nSims) {
my.data = sample(coin, size = 10, replace = TRUE, prob = c(0.75, 0.25))
number.heads[i] = sum(my.data == "H")
}
number.heads
mean(number.heads)
nSims = 10000
coin = c("H", "T")
number.heads = rep(NA, nSims)
for (i in 1:nSims) {
my.data = sample(coin, size = 10, replace = TRUE, prob = c(0.75, 0.25))
number.heads[i] = sum(my.data == "H")
}
mean(number.heads)
nSims = 10000
coin = c("H", "T")
number.heads = rep(NA, nSims)
for (i in 1:nSims) {
my.data = sample(coin, size = 10, replace = TRUE, prob = c(0.65, 0.35))
number.heads[i] = sum(my.data == "H")
}
mean(number.heads)
children = sample(q1, size = 1)
q1 = c("GB", "GG")
children = sample(q1, size = 1)
children
head(airquality)
head(airquality)
table(airquality$Month)
mean(airquality$Wind)
median(airquality$Wind)
sd(airquality$Wind)
head(airquality)
mean(airquality$Wind[airquality$Month == 6])
median(airquality$Wind[airquality$Month == 6])
sd(airquality$Wind[airquality$Month == 6])
sum(airquality$Wind > 12)
sum(airquality$Wind > 12)
nrow(airquality)
sum(airquality$Wind > 12) / nrow(airquality)
mean(quality$Wind > 12)
mean(airquality$Wind > 12)
airquality$Wind
airquality$Wind > 12
mean(airquality$Wind > 12)
as.numeric(airquality$Wind > 12)
mean(airquality$Wind > 12)
setwd("~/research/global_gp")
library(ggplot2)
library(spBayes)
library(MBA)
library(fields)
library(latex2exp)
nScen <- 6
nReps <- 10
line.type <- 2
line.width <- 4
nTest <- 25
# Surface plots for beta0
pdf("figures/svc/beta0_svc.pdf", width = 8, height = 6)
par(mfrow = c(2,3), mar = c(3, 4, 2, 2) + 0.1, oma = c(0, 0, 4, 0))
for (i in 1:nScen) {
path <- paste0("objects/svc_scen", i, ".RDS")
results <- readRDS(path)
test <- readRDS(paste0("data/small/scen", i, "/test.RDS"))
beta.mu <- mean(results[[1]]$preds$p.beta.recover.samples[ , 1])
w.mu <- apply(results[[1]]$preds$p.w.predictive.samples[1:nTest, ], 1, mean)
beta0.means <- beta.mu + w.mu
mba.data <- data.frame(test$U, beta0.means)
mba.interp <- mba.surf(mba.data, no.X=100, no.Y=100, extend=TRUE)
image(mba.interp$xyz.est, main = paste0("Scen. ", i),
col = tim.colors(64), cex.main = 1.5)
mtext(TeX("$\\beta_0"), side = 3, line = 1, outer = TRUE, cex = 1.5)
}
dev.off()
# Surface plots for beta1
pdf("figures/svc/beta1_svc.pdf", width = 8, height = 6)
par(mfrow = c(2,3), mar = c(3, 4, 2, 2) + 0.1, oma = c(0, 0, 4, 0))
for (i in 1:nScen) {
path <- paste0("objects/svc_scen", i, ".RDS")
results <- readRDS(path)
test <- readRDS(paste0("data/small/scen", i, "/test.RDS"))
beta.mu <- mean(results[[1]]$preds$p.beta.recover.samples[ , 2])
w.mu <- apply(results[[1]]$preds$p.w.predictive.samples[(nTest+1):(2*nTest), ], 1, mean)
beta1.means <- beta.mu + w.mu
mba.data <- data.frame(test$U, beta1.means)
mba.interp <- mba.surf(mba.data, no.X=100, no.Y=100, extend=TRUE)
image(mba.interp$xyz.est, main = paste0("Scen. ", i),
col = tim.colors(64), cex.main = 1.5)
mtext(TeX("$\\beta_1"), side = 3, line = 1, outer = TRUE, cex = 1.5)
}
dev.off()
# Surface plots for beta2
pdf("figures/svc/beta2_svc.pdf", width = 8, height = 6)
par(mfrow = c(2,3), mar = c(3, 4, 2, 2) + 0.1, oma = c(0, 0, 4, 0))
for (i in 1:nScen) {
path <- paste0("objects/svc_scen", i, ".RDS")
results <- readRDS(path)
test <- readRDS(paste0("data/small/scen", i, "/test.RDS"))
beta.mu <- mean(results[[1]]$preds$p.beta.recover.samples[ , 3])
w.mu <- apply(results[[1]]$preds$p.w.predictive.samples[(2*nTest+1):(3*nTest), ], 1, mean)
beta2.means <- beta.mu + w.mu
mba.data <- data.frame(test$U, beta2.means)
mba.interp <- mba.surf(mba.data, no.X=100, no.Y=100, extend=TRUE)
image(mba.interp$xyz.est, main = paste0("Scen. ", i),
col = tim.colors(64), cex.main = 1.5)
mtext(TeX("$\\beta_2"), side = 3, line = 1, outer = TRUE, cex = 1.5)
}
dev.off()
# Density plots for tau2
pdf("figures/svc/tau2_svc.pdf", width = 8, height = 6)
par(mfrow = c(2,3))
for (i in 1:nScen) {
path <- paste0("objects/svc_scen", i, ".RDS")
results <- readRDS(path)
tau2_samples <- results[[1]]$model$p.theta.samples[ , 4]
if (i == 4) {
true_tau2 <- 2
} else {
true_tau2 <- 0.2
}
hist(tau2_samples,
xlab = paste0("Scenario ", i),
main = "", ylab = "", cex.lab = 1.75)
abline(v = true_tau2, lty = line.type, lwd = line.width, col = "blue")
mtext(TeX("$\\tau^2$"), side = 3, line = -2.5, outer = TRUE, cex = 1.5)
}
dev.off()
### BOXPLOTS FOR PREDICTIVE DIAGNOSTICS ###
# Get diagnostics (rmse, coverage, length)
rmse <- cvg <- len <- c()
for (i in 1:nScen) {
path <- paste0("objects/svc_scen", i, ".RDS")
results <- readRDS(path)
test <- readRDS(paste0("data/small/scen", i, "/test.RDS"))
STest <- nrow(test$Z)
nTest <- nrow(test$X)
a <- .05
for (j in 1:nReps) {
rmse_vec <- cvg_vec <- len_vec <- numeric(STest)
for (k in 1:STest) {
truth <- test$Y[(nTest*(k-1)+1):(nTest*k), ]
pred.samples <- results[[j]]$preds$p.y.predictive.samples
preds <- apply(pred.samples, 1, mean)
rmse_vec[k] <- sqrt(mean((truth - preds)^2))
lower <- apply(pred.samples, 1, quantile, .025)
upper <- apply(pred.samples, 1, quantile, .975)
cvg_vec[k] <- mean(lower < truth & upper > truth)
len_vec[k] <- mean(upper - lower)
}
rmse_vec <- mean(rmse_vec)
cvg_vec <- mean(cvg_vec)
len_vec <- mean(len_vec)
rmse <- c(rmse, rmse_vec)
cvg <- c(cvg, cvg_vec)
len <- c(len, len_vec)
}
}
