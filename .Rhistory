duo_harris
par(mar=c(7, 7, 4, 2))
barplot(duo_harris$win_prob_harris,
xlab = "", ylab = "",
cex.main = 1.5,
ylim = c(0, 1),
yaxt = 'n',
main = "Harris",
col = terrain.colors(21))
abline(h = p_win_harris, lty = 2)
axis(2, at = seq(0, 1, by=0.2), labels = paste0(seq(0, 100, by=20), "%"), las = 2)
axis(1, at = seq(0.8, 24.75, length=21), labels = paste0(toupper(duo_harris$state1), ", ",
toupper(duo_harris$state2)), las = 2)
title(xlab = "Equal Probability Scenario (1)",
ylab = "Conditional Win Probability",
cex.lab = 1.4, line = 5)
# Trump
trump_duo_probs <- sapply(1:nrow(state_duos), function(x) {
conditionalProbMulti(states = state_duos[x, ], win = TRUE, candidate = "Trump")
})
duo_trump <- data.frame(state1 = state_duos[,1],
state2 = state_duos[,2],
win_prob_trump = trump_duo_probs) %>%
arrange(-win_prob_trump)
duo_trump
par(mar=c(7, 7, 4, 2))
barplot(duo_trump$win_prob_trump,
xlab = "", ylab = "",
cex.main = 1.5,
ylim = c(0, 1),
yaxt = 'n',
main = "Trump",
col = terrain.colors(21))
abline(h = p_win_trump, lty = 2)
axis(2, at = seq(0, 1, by=0.2), labels = paste0(seq(0, 100, by=20), "%"), las = 2)
axis(1, at = seq(0.8, 24.75, length=21), labels = paste0(toupper(duo_trump$state1), ", ",
toupper(duo_trump$state2)), las = 2)
title(xlab = "Equal Probability Scenario (1)",
ylab = "Conditional Win Probability",
cex.lab = 1.4, line = 5)
wpd
biden <- c(154.1, 121.5, 98.6, 45.4, 62.7, 59, 4, 20.7, 8.4, 3.3)
trump <- c(103.4, 74.2, 21.5, 65.4, 38.8, 38.4, 24.5, 6.8, 10.5, 12.3)
spending <- rbind(biden, trump)
colnames(spending) <- c("FL", "PA", "MI", "NC", "WI", "AZ", "GA", "NV", "OH", "IA")
rownames(spending) <- c("Biden", "Trump")
par(mar=c(5, 6, 2, 2))
barplot(spending,
col = c("dodgerblue", "brown3"),
legend = rownames(wpd),
xlab = "Equal Probability Scenario (1)",
ylab = "Win Probability Difference \n (WPD)",
yaxt = 'n',
cex.lab = 1.5,
beside = TRUE)
barplot(spending,
col = c("dodgerblue", "brown3"),
legend = rownames(spending),
xlab = "Equal Probability Scenario (1)",
ylab = "Win Probability Difference \n (WPD)",
yaxt = 'n',
cex.lab = 1.5,
beside = TRUE)
barplot(spending,
col = c("dodgerblue", "brown3"),
legend = rownames(spending),
xlab = "State",
ylab = "TV Ad Spending (Millions USD)",
yaxt = 'n',
cex.lab = 1.5,
beside = TRUE)
biden <- c(154.1, 121.5, 98.6, 45.4, 62.7, 59, 4, 20.7, 8.4, 3.3)
trump <- c(103.4, 74.2, 21.5, 65.4, 38.8, 38.4, 24.5, 6.8, 10.5, 12.3)
spending <- rbind(biden, trump)
colnames(spending) <- c("FL", "PA", "MI", "NC", "WI", "AZ", "GA", "NV", "OH", "IA")
rownames(spending) <- c("Biden", "Trump")
par(mar=c(5, 6, 2, 2))
barplot(spending,
col = c("dodgerblue", "brown3"),
legend = rownames(spending),
xlab = "2020 Election",
ylab = "TV Ad Spending \n(Millions USD)",
yaxt = 'n',
cex.lab = 1.5,
beside = TRUE)
axis(2, at = seq(0, 0.6, by=0.1), labels = paste0(seq(0, 60, by=10), "%"), las=2)
biden <- c(154.1, 121.5, 98.6, 45.4, 62.7, 59, 4, 20.7, 8.4, 3.3)
trump <- c(103.4, 74.2, 21.5, 65.4, 38.8, 38.4, 24.5, 6.8, 10.5, 12.3)
spending <- rbind(biden, trump)
colnames(spending) <- c("FL", "PA", "MI", "NC", "WI", "AZ", "GA", "NV", "OH", "IA")
rownames(spending) <- c("Biden", "Trump")
par(mar=c(5, 6, 2, 2))
barplot(spending,
col = c("dodgerblue", "brown3"),
legend = rownames(spending),
xlab = "2020 Election",
ylab = "TV Ad Spending \n(Millions USD)",
#yaxt = 'n',
cex.lab = 1.5,
beside = TRUE)
#axis(2, at = seq(0, 0.6, by=0.1), labels = paste0(seq(0, 60, by=10), "%"), las=2)
harris <- c(109, 81, 58, 49, 44, 28, 24)
trump <- c(102, 18, 29, 15, 17, 4, 3)
spending <- rbind(harris, trump)
colnames(spending) <- c("PA", "MI", "GA", "WI", "AZ", "NC", "NV")
rownames(spending) <- c("Harris", "Trump")
par(mar=c(5, 6, 2, 2))
barplot(spending,
col = c("dodgerblue", "brown3"),
legend = rownames(spending),
xlab = "2024 Election",
ylab = "TV Ad Spending \n(Millions USD)",
cex.lab = 1.5,
beside = TRUE)
setwd("~/research/global_gp/compressed_mcmc")
setwd("~/research/global_gp")
# SOURCES
source("compressed_mcmc/mcmc.R") # Metropolis-Gibbs Sampler
source("compressed_mcmc/priors.R")
source("compressed_mcmc/jacobians.R")
source("compressed_mcmc/likelihood.R")
source("compressed_mcmc/posterior.R")
source("other_functions/helper_functions.R") # Other misc functions (not part of MCMC)
source("other_functions/bsplines_2_3D.R")
library(fields)
load("data/train.RData")
load("data/test.RData")
load("data/theta.RData")
n <- nrow(train$X)
nTest <- nrow(test$X)
S <- nrow(train$Z)
STest <- nrow(test$Z)
X <- train$X
Z <- train$Z
Y <- matrix(train$Y, nrow = n, ncol = S)
U <- train$U
D <- train$D
XTest <- test$X
ZTest <- test$Z
YTest <- test$Y
UTest <- test$U
DTest <- test$D
K <- 9
propSD <- list(sigf2 = 0.3,
thf = 0.7,
sigma2 = seq(0.05, 0.15, length = K),
tau2 = 0.4,
theta = seq(0.2, 0.5, length = K))
starting <- list(sigma2 = seq(50, 100, length = K),
theta = rep(0.5, K),
sigf2 = 6,
thf = 1.3,
tau2 = 0.1,
beta = c(3, 0, 0))
results <- mcmc(X = X, Z = Z, Y = Y, D = D, K = K,
starting = starting,
propSD = propSD,
nIter = 10, nBurn = 10, nThin=2,
model = "full_gp")
traceback()
basis
basis[[1]]
dim(basis[[1]])
dim(D)
YTest <- test$Y
# SOURCES
source("compressed_mcmc/mcmc.R") # Metropolis-Gibbs Sampler
source("compressed_mcmc/priors.R")
source("compressed_mcmc/jacobians.R")
source("compressed_mcmc/likelihood.R")
source("compressed_mcmc/posterior.R")
source("other_functions/helper_functions.R") # Other misc functions (not part of MCMC)
source("other_functions/bsplines_2_3D.R")
library(fields)
load("data/train.RData")
load("data/test.RData")
load("data/theta.RData")
n <- nrow(train$X)
nTest <- nrow(test$X)
S <- nrow(train$Z)
STest <- nrow(test$Z)
X <- train$X
Z <- train$Z
Y <- matrix(train$Y, nrow = n, ncol = S)
U <- train$U
D <- train$D
XTest <- test$X
ZTest <- test$Z
YTest <- test$Y
UTest <- test$U
DTest <- test$D
K <- 9
propSD <- list(sigf2 = 0.3,
thf = 0.7,
sigma2 = seq(0.05, 0.15, length = K),
tau2 = 0.4,
theta = seq(0.2, 0.5, length = K))
starting <- list(sigma2 = seq(50, 100, length = K),
theta = rep(0.5, K),
sigf2 = 6,
thf = 1.3,
tau2 = 0.1,
beta = c(3, 0, 0))
results <- mcmc(X = X, Z = Z, Y = Y, D = D, K = K,
starting = starting,
propSD = propSD,
nIter = 10, nBurn = 10, nThin=2,
model = "full_gp")
traceback()
dim(phi)
starting
starting$sigma2
length(B)
dim(B[[1]])
(Reduce("+", lapply(1:K, \(k) starting$sigma2[k] * B[[k]])) +
starting$sigf2 * exp(-starting$thf * DXFull))
dim(DXFull)
# SOURCES
source("compressed_mcmc/mcmc.R") # Metropolis-Gibbs Sampler
source("compressed_mcmc/priors.R")
source("compressed_mcmc/jacobians.R")
source("compressed_mcmc/likelihood.R")
source("compressed_mcmc/posterior.R")
source("other_functions/helper_functions.R") # Other misc functions (not part of MCMC)
source("other_functions/bsplines_2_3D.R")
library(fields)
load("data/train.RData")
load("data/test.RData")
load("data/theta.RData")
n <- nrow(train$X)
nTest <- nrow(test$X)
S <- nrow(train$Z)
STest <- nrow(test$Z)
X <- train$X
Z <- train$Z
Y <- matrix(train$Y, nrow = n, ncol = S)
U <- train$U
D <- train$D
XTest <- test$X
ZTest <- test$Z
YTest <- test$Y
UTest <- test$U
DTest <- test$D
K <- 9
propSD <- list(sigf2 = 0.3,
thf = 0.7,
sigma2 = seq(0.05, 0.15, length = K),
tau2 = 0.4,
theta = seq(0.2, 0.5, length = K))
starting <- list(sigma2 = seq(50, 100, length = K),
theta = rep(0.5, K),
sigf2 = 6,
thf = 1.3,
tau2 = 0.1,
beta = c(3, 0, 0))
results <- mcmc(X = X, Z = Z, Y = Y, D = D, K = K,
starting = starting,
propSD = propSD,
nIter = 10, nBurn = 10, nThin=2,
model = "full_gp")
traceback()
(Reduce("+", lapply(1:K, \(k) starting$sigma2[k] * B[[k]])) +
starting$sigf2 * exp(-starting$thf * DXFull))
dim((Reduce("+", lapply(1:K, \(k) starting$sigma2[k] * B[[k]])) +
starting$sigf2 * exp(-starting$thf * DXFull)))
dim(phi)
m <- matrix(1:4, 2, 2)
m
m %x% c(1,1)
c(1,1) %x% m
dim(phi)
matrix(1, 1, 2) %x% m
matrix(1, 1, 2)
matrix(1, 1, 2) %x% m
# SOURCES
source("compressed_mcmc/mcmc.R") # Metropolis-Gibbs Sampler
source("compressed_mcmc/priors.R")
source("compressed_mcmc/jacobians.R")
source("compressed_mcmc/likelihood.R")
source("compressed_mcmc/posterior.R")
source("other_functions/helper_functions.R") # Other misc functions (not part of MCMC)
source("other_functions/bsplines_2_3D.R")
library(fields)
load("data/train.RData")
load("data/test.RData")
load("data/theta.RData")
n <- nrow(train$X)
nTest <- nrow(test$X)
S <- nrow(train$Z)
STest <- nrow(test$Z)
X <- train$X
Z <- train$Z
Y <- matrix(train$Y, nrow = n, ncol = S)
U <- train$U
D <- train$D
XTest <- test$X
ZTest <- test$Z
YTest <- test$Y
UTest <- test$U
DTest <- test$D
K <- 9
propSD <- list(sigf2 = 0.3,
thf = 0.7,
sigma2 = seq(0.05, 0.15, length = K),
tau2 = 0.4,
theta = seq(0.2, 0.5, length = K))
starting <- list(sigma2 = seq(50, 100, length = K),
theta = rep(0.5, K),
sigf2 = 6,
thf = 1.3,
tau2 = 0.1,
beta = c(3, 0, 0))
results <- mcmc(X = X, Z = Z, Y = Y, D = D, K = K,
starting = starting,
propSD = propSD,
nIter = 10, nBurn = 10, nThin=2,
model = "full_gp")
traceback()
dim(phiFull)
# SOURCES
source("compressed_mcmc/mcmc.R") # Metropolis-Gibbs Sampler
source("compressed_mcmc/priors.R")
source("compressed_mcmc/jacobians.R")
source("compressed_mcmc/likelihood.R")
source("compressed_mcmc/posterior.R")
source("other_functions/helper_functions.R") # Other misc functions (not part of MCMC)
source("other_functions/bsplines_2_3D.R")
library(fields)
load("data/train.RData")
load("data/test.RData")
load("data/theta.RData")
n <- nrow(train$X)
nTest <- nrow(test$X)
S <- nrow(train$Z)
STest <- nrow(test$Z)
X <- train$X
Z <- train$Z
Y <- matrix(train$Y, nrow = n, ncol = S)
U <- train$U
D <- train$D
XTest <- test$X
ZTest <- test$Z
YTest <- test$Y
UTest <- test$U
DTest <- test$D
K <- 9
propSD <- list(sigf2 = 0.3,
thf = 0.7,
sigma2 = seq(0.05, 0.15, length = K),
tau2 = 0.4,
theta = seq(0.2, 0.5, length = K))
starting <- list(sigma2 = seq(50, 100, length = K),
theta = rep(0.5, K),
sigf2 = 6,
thf = 1.3,
tau2 = 0.1,
beta = c(3, 0, 0))
results <- mcmc(X = X, Z = Z, Y = Y, D = D, K = K,
starting = starting,
propSD = propSD,
nIter = 10, nBurn = 10, nThin=2,
model = "full_gp")
traceback()
dim(phiFull)
dim(Reduce("+", lapply(1:K, \(k) starting$sigma2[k] * B[[k]])) +
starting$sigf2 * exp(-starting$thf * DXFull))
dim(t(phiFull))
m
### Wasserstein mean for multiple Markov chains (multiple distributions)
wasserstein <- function(results, time) {
wassersteinAcc <- rowMeans(sapply(results, \(x) unlist(x$acceptance)))
wassersteinMeans <- rowMeans(sapply(results, \(x) unlist(x$posteriorMedians)))
wassersteinLower <- rowMeans(sapply(results, \(x) unlist(x$credLower)))
wassersteinUpper <- rowMeans(sapply(results, \(x) unlist(x$credUpper)))
predictions <- vector("list", nTestSubj)
for (i in 1:nTestSubj) {
predsList <- lapply(results, \(x) x$preds[[i]])
predictions[[i]] <- Reduce("+", predsList) / length(predsList)
}
wassersteinResults <- list(acc = wassersteinAcc,
means = wassersteinMeans,
lower = wassersteinLower,
upper = wassersteinUpper,
predictions = predictions,
time = time)
}
# Energy score (CRPS) calculation for predictions vs truth
energy_score = function(y, z){
d = dim(z)
n_samp = d[1]
n = d[2]
G = colMeans(sapply(1:n,function(i) sapply(1:n_samp, function(k) norm(z[k,i]-y[i],type='2'))))
UQ = sapply(1:n,function(i) (1/(2*n_samp^2))*sum(as.matrix(distances::distances(z[,i]))))
return(G - UQ)
}
# Calculates the base of the covariance matrix for likelihood function
baseVariance <- function(theta, D) {
B <- lapply(1:K, \(k) tcrossprod(basis[[k]] %*% exp(-theta[k] * D), basis[[k]]))
return(B)
}
# Convert list of cluster labels to vector of indices (for data subsetting)
list2Vec <- function(ls) {
temp <- rep(seq_along(ls), lengths(ls))
temp[unlist(ls)]
}
# SOURCES
source("compressed_mcmc/mcmc.R") # Metropolis-Gibbs Sampler
source("compressed_mcmc/priors.R")
source("compressed_mcmc/jacobians.R")
source("compressed_mcmc/likelihood.R")
source("compressed_mcmc/posterior.R")
source("other_functions/helper_functions.R") # Other misc functions (not part of MCMC)
source("other_functions/bsplines_2_3D.R")
library(fields)
load("data/train.RData")
load("data/test.RData")
load("data/theta.RData")
n <- nrow(train$X)
nTest <- nrow(test$X)
S <- nrow(train$Z)
STest <- nrow(test$Z)
X <- train$X
Z <- train$Z
Y <- matrix(train$Y, nrow = n, ncol = S)
U <- train$U
D <- train$D
XTest <- test$X
ZTest <- test$Z
YTest <- test$Y
UTest <- test$U
DTest <- test$D
K <- 9
propSD <- list(sigf2 = 0.3,
thf = 0.7,
sigma2 = seq(0.05, 0.15, length = K),
tau2 = 0.4,
theta = seq(0.2, 0.5, length = K))
starting <- list(sigma2 = seq(50, 100, length = K),
theta = rep(0.5, K),
sigf2 = 6,
thf = 1.3,
tau2 = 0.1,
beta = c(3, 0, 0))
results <- mcmc(X = X, Z = Z, Y = Y, D = D, K = K,
starting = starting,
propSD = propSD,
nIter = 10, nBurn = 10, nThin=2,
model = "full_gp")
#theta
mean(train$Y)
sd(train$Y)
results$posteriorMeans
results$acceptance
nSamples <- length(results$paramSamples[[5]])
plot(1:nSamples, results$paramSamples[[5]], type="l")
saveRDS(results, file = "objects/nngp.RDS")
library(MBA)
library(fields)
lims <- c(-15, 15)
# pdf("figures/subj1_true.pdf")
# pred.surf <-  mba.surf(cbind(UTest, YTest[1:nTest]), no.X=100, no.Y=100, extend=T)$xyz.est
# image.plot(pred.surf, xaxs ="r", yaxs = "r", zlim = lims, main="True Surface, Subject 1",
#            cex.main = 1.5, col = hcl.colors(12, "YlOrRd", rev=TRUE))
# contour(pred.surf, add=T)
# dev.off()
#
# pdf("figures/subj1_global.pdf")
# pred.surf <-  mba.surf(cbind(UTest, results$preds[2,1:nTest]), no.X=100, no.Y=100, extend=T)$xyz.est
# image.plot(pred.surf, xaxs ="r", yaxs = "r", zlim = lims, main="Global GP, Subject 1",
#            cex.main = 1.5, col = hcl.colors(12, "YlOrRd", rev=TRUE))
# contour(pred.surf, add=T)
# dev.off()
#
# pdf("figures/subj2_true.pdf")
# pred.surf <-  mba.surf(cbind(UTest, YTest[(nTest+1):(2*nTest)]), no.X=100, no.Y=100, extend=T)$xyz.est
# image.plot(pred.surf, xaxs ="r", yaxs = "r", zlim = lims, main="True Surface, Subject 2",
#            cex.main = 1.5, col = hcl.colors(12, "YlOrRd", rev=TRUE))
# contour(pred.surf, add=T)
# dev.off()
#
# pdf("figures/subj2_global.pdf")
# pred.surf <-  mba.surf(cbind(UTest, results$preds[2, (nTest+1):(2*nTest)]), no.X=100, no.Y=100, extend=T)$xyz.est
# image.plot(pred.surf, xaxs ="r", yaxs = "r", zlim = lims, main="Global GP, Subject 2",
#            cex.main = 1.5, col = hcl.colors(12, "YlOrRd", rev=TRUE))
# contour(pred.surf, add=T)
# dev.off()
rmse <- cvg <- width <- scores <- crps <- numeric(STest)
a <- .05
for (i in 1:STest) {
truth <- YTest[(nTest*(i-1)+1):(i*nTest)]
pred <- results$preds[2, (nTest*(i-1)+1):(i*nTest)]
rmse[i] <- sqrt(mean((truth - pred)^2))
lower <- results$preds[1, (nTest*(i-1)+1):(i*nTest)]
upper <- results$preds[3, (nTest*(i-1)+1):(i*nTest)]
cvg[i] <- mean(lower < truth & upper > truth)
width[i] <- mean(upper - lower)
scores[i] <- mean((upper - lower) +
2/a * (lower - truth) * (truth < lower) +
2/a * (truth - upper) * (truth > upper))
predSamples <- t(results$predSamples[(nTest*(i-1)+1):(i*nTest), ])
crps[i] <- mean(energy_score(truth, predSamples))
}
rmse
cat(paste0("Root MS error: ", round(mean(rmse), 3), "\n"))
cvg
cat(paste0("Mean coverage: ", round(mean(cvg), 3), "\n"))
width
cat(paste0("Mean width: ", round(mean(width), 3), "\n"))
scores
cat(paste0("Mean interval score: ", round(mean(scores), 3), "\n"))
crps
cat(paste0("Mean CRPS: ", round(mean(crps), 3), "\n"))
