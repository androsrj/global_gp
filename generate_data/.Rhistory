mean(my.samples[2,])
n.sims <- 100
my.samples <- rmultinom(n.sims, 5, c(0.3,0.5,0.2))
mean(my.samples[2,])
# Simulation from HW Q15
n.sims <- 10000
patients <- rmultinom(n.sims, 100, c(0.25, 0.35, 0.40))
# Simulation from HW Q15
n.sims <- 10000
patients <- rmultinom(n.sims, 100, c(0.25, 0.35, 0.40))
patients[ , 1:10]
patients[1,]
patients[1,] < 20
mean(patients[1,] < 20)
pbinom(19, 100, .25)
a=5
satellites <- c(8, 0, 9, 0, 4, 0, 0, 0, 0, 0, 0, 0, 11, 0, 14, 8, 1, 1, 0, 5,
4, 3, 1, 2, 3, 0, 3, 5, 0, 0, 4, 0, 0, 0, 8, 5, 0, 0, 6, 0, 6,
3, 5, 6, 3, 6, 5, 6, 5, 9, 4, 6, 15, 3, 3, 3, 0, 0, 5, 0, 0, 5,
0, 0, 4, 0, 0, 3, 4, 0, 5, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 5,
0, 0, 0, 0, 0, 0, 0, 0)
# Define the Poisson log likelihood function
poisson_log_likelihood <- function(lambda, data) {
sum(dpois(data, lambda, log = TRUE))
}
# Generate lambda sequence from 1 to 5 by 0.1
lambdas <- seq(1, 5, by = 0.1)
# Calculate log likelihood for each lambda value
log_likelihoods <- sapply(lambdas, poisson_log_likelihood, data = satellites)
# Plot results
plot(lambdas, log_likelihoods, type = "o", pch = 16,
xlab = expression(lambda),
ylab = "Log Likelihood",
main = "Log Likelihood vs Lambda for Poisson Distribution")
grid()
# Plot results
plot(lambdas, log_likelihoods, type = "o", pch = 16,
xlab = expression(lambda),
ylab = "Log Likelihood",
main = "Log Likelihood vs Lambda for Poisson Distribution")
setwd("~/research/elections_general")
# Install and load necessary packages
library(sf)
library(dplyr)
library(spdep)
# Step 1: Download and read the shapefile for the 48 contiguous U.S. states
#url <- "https://www2.census.gov/geo/tiger/GENZ2021/shp/cb_2021_us_state_20m.zip"
#download.file(url, destfile = "us_states.zip")
#unzip("us_states.zip", exdir = "us_states")
states <- st_read("us_states/cb_2021_us_state_20m.shp")
# Keep only contiguous 48 states
states <- states %>% filter(!STUSPS %in% c("HI", "AK"))
# Make geometries valid
states <- st_make_valid(states) %>% st_transform(5070)
states$NAME
# Test case
ind.a <- which(states$NAME == "Arkansas")
ind.b <- which(states$NAME == "Mississippi")
a <- states[ind.a, ]
b <- states[ind.b, ]
# Shared border length
border <- st_intersection(st_boundary(a), st_boundary(b))
as.numeric(st_length(border)) / 1000 * 0.621371
# Get neighbors
neighbors <- st_touches(states)
# Initialize weight list
weights_list <- vector("list", length(neighbors))
for (i in seq_along(neighbors)) {
w <- numeric(length(neighbors[[i]]))
for (j_idx in seq_along(neighbors[[i]])) {
j <- neighbors[[i]][j_idx]
# Compute shared border
shared <- st_intersection(st_geometry(states[i,]), st_geometry(states[j,]))
if (length(shared) > 0) {
w[j_idx] <- as.numeric(sum(st_length(shared))) # raw length in meters
} else {
w[j_idx] <- 0
}
}
weights_list[[i]] <- w
}
weights_list
# Initialize weight list
weights_list <- vector("list", length(neighbors))
for (i in seq_along(neighbors)) {
w <- numeric(length(neighbors[[i]]))
for (j_idx in seq_along(neighbors[[i]])) {
j <- neighbors[[i]][j_idx]
# Compute shared border
shared <- st_intersection(st_geometry(states[i,]), st_geometry(states[j,]))
if (length(shared) > 0) {
w[j_idx] <- as.numeric(sum(st_length(shared))) # raw length in meters
} else {
w[j_idx] <- 0
}
}
weights_list[[i]] <- w / 1000 * 0.621371
}
weights_list[[1]]
states$NAME
# Step 1: Download and read the shapefile for the 48 contiguous U.S. states
#url <- "https://www2.census.gov/geo/tiger/GENZ2021/shp/cb_2021_us_state_20m.zip"
#download.file(url, destfile = "us_states.zip")
#unzip("us_states.zip", exdir = "us_states")
states <- st_read("us_states/cb_2021_us_state_20m.shp")
# Keep only contiguous 48 states and sort alphabetically
states <- states %>%
filter(!STUSPS %in% c("HI", "AK")) %>%
arrange(NAME)
head(STATES)
head(states)
# Make geometries valid
states <- st_make_valid(states) %>% st_transform(5070)
# Get neighbors
neighbors <- st_touches(states)
# Initialize weight list
weights_list <- vector("list", length(neighbors))
for (i in seq_along(neighbors)) {
w <- numeric(length(neighbors[[i]]))
for (j_idx in seq_along(neighbors[[i]])) {
j <- neighbors[[i]][j_idx]
# Compute shared border
shared <- st_intersection(st_geometry(states[i,]), st_geometry(states[j,]))
if (length(shared) > 0) {
w[j_idx] <- as.numeric(sum(st_length(shared))) # raw length in meters
} else {
w[j_idx] <- 0
}
}
weights_list[[i]] <- w / 1000 * 0.621371
}
weights_list[[1]]
weights_list[[2]]
states$NAME
weights_list[[46]]
weights[[3]]
weights_list[[3]]
vec=1:3
names(vec)
names(vec[1]="A")
names(vec[1]) = "A"
vec
names(vec)[1] = "A"
vec
# Initialize weight list
weights_list <- vector("list", length(neighbors))
for (i in seq_along(neighbors)) {
w <- numeric(length(neighbors[[i]]))
for (j_idx in seq_along(neighbors[[i]])) {
j <- neighbors[[i]][j_idx]
# Compute shared border
shared <- st_intersection(st_geometry(states[i,]), st_geometry(states[j,]))
if (length(shared) > 0) {
w[j_idx] <- as.numeric(sum(st_length(shared))) # raw length in meters
} else {
w[j_idx] <- 0
names(w)[j_idx] <- states$NAME[j]
}
}
weights_list[[i]] <- w / 1000 * 0.621371
}
weights_list[[1]]
# Initialize weight list
weights_list <- vector("list", length(neighbors))
for (i in seq_along(neighbors)) {
w <- numeric(length(neighbors[[i]]))
for (j_idx in seq_along(neighbors[[i]])) {
j <- neighbors[[i]][j_idx]
# Compute shared border
shared <- st_intersection(st_geometry(states[i,]), st_geometry(states[j,]))
if (length(shared) > 0) {
w[j_idx] <- as.numeric(sum(st_length(shared))) # raw length in meters
} else {
w[j_idx] <- 0
}
names(w)[j_idx] <- states$NAME[j]
}
weights_list[[i]] <- w / 1000 * 0.621371
}
weights_list[[1]]
weights_list[[3]]
states$NAME
weights_list[[17]]
weights_list[[3]]
states$NAME
weights[[27]]
weights_list[[27]]
names(weights_list) <- states$NAME
weights_list$Illinois
which.max(unlist(weights_list))
weights_list$Oklahoma
weights_list$Texas
mean(1,2,3,4,5)
x=c(10, "three")
class(x)
x
x = rpois(200, 2.9)
dnorm(x, 2.1, log = TRUE)
sum(dnorm(x, 2.1, log = TRUE))
sum(dpois(x, 2.1, log = TRUE))
data(ToothGrowth)
head(ToothGrowth)
mean(ToothGrowth$len)
mean(ToothGrowth$len[ToothGrowth$supp == "OJ"])
mean(ToothGrowth$len[ToothGrowth$dose == 0.5])
View(ToothGrowth)
tooth.oj = ToothGrowth[ToothGrowth$supp == "OJ", ]
mean(tooth.oj$len)
tooth.oj = ToothGrowth[ToothGrowth$supp == "OJ", ]
mean(tooth.oj$len)
# same as this:
mean(ToothGrowth$len[ToothGrowth$supp == "OJ"])
?dmutinom
?dmultinom
dmultinom(c(8, 1, 1), 10, c(0.7, 0.2, 0.1))
rmultinom(6, size = 10, prob = c(0.7, 0.2, 0.1))
rmultinom(6, size = 10, prob = c(0.7, 0.2, 0.1))
setwd("~/research/global_gp/generate_data")
setwd("~/research/global_gp/generate_data")
setwd("~/research/global_gp/generate_data")
source("../other_functions/spatial_data_scen11.R")
mySeed <- 45213
# Sample sizes
# Can have a "small" dataset with n = 100 and nTest = 25
# Then a "large" dataset with n = 500 and nTest = 100
n <- 100
nTest <- 25
# Number of subjects - can probably leave these alone
S <- 10
STest <- 10
# Number of predictors
p <- 2
# Covariance parameters for beta
trueSigb2 <- seq(0.5, 1, length = p + 1)
trueThb <- seq(0.1, 0.2, length = p + 1)
# Covariance parameters for global covariates (each length K)
trueSigma2 <- seq(5, 10, length = K)
##########################
# Generate training data #
set.seed(mySeed)
X <- matrix(runif(n*p, 0, 10), nrow = n, ncol = p)
# Number of predictors
K <- 9
p <- 2
# Covariance parameters for beta
trueSigb2 <- seq(0.5, 1, length = p + 1)
trueThb <- seq(0.1, 0.2, length = p + 1)
# Covariance parameters for global covariates (each length K)
trueSigma2 <- seq(5, 10, length = K)
trueTheta <- seq(0.1, 0.5, length = K)
# Error variance
trueTau2 <- 0.2
##########################
# Generate training data #
set.seed(mySeed)
X <- matrix(runif(n*p, 0, 10), nrow = n, ncol = p)
Z <- matrix(runif(2 * S, 0, 100), ncol = 2)
train <- spatialData(n = n,
X = X,
Z = Z,
sigb2 = trueSigb2,
thb = trueThb,
sigma2 = trueSigma2,
theta = trueTheta,
tau2 = trueTau2,
range = c(0, 100))
train <- spatialData(n = n,
X = X,
Z = Z,
K = K,
sigb2 = trueSigb2,
thb = trueThb,
sigma2 = trueSigma2,
theta = trueTheta,
tau2 = trueTau2,
range = c(0, 100))
source("../other_functions/spatial_data_scen11.R")
source("../other_functions/bsplines_2_3D.R")
mySeed <- 45213
# Sample sizes
# Can have a "small" dataset with n = 100 and nTest = 25
# Then a "large" dataset with n = 500 and nTest = 100
n <- 100
nTest <- 25
# Number of subjects - can probably leave these alone
S <- 10
STest <- 10
# Number of predictors
K <- 9
p <- 2
# Covariance parameters for beta
trueSigb2 <- seq(0.5, 1, length = p + 1)
trueThb <- seq(0.1, 0.2, length = p + 1)
# Covariance parameters for global covariates (each length K)
trueSigma2 <- seq(5, 10, length = K)
trueTheta <- seq(0.1, 0.5, length = K)
# Error variance
trueTau2 <- 0.2
##########################
# Generate training data #
set.seed(mySeed)
X <- matrix(runif(n*p, 0, 10), nrow = n, ncol = p)
Z <- matrix(runif(2 * S, 0, 100), ncol = 2)
train <- spatialData(n = n,
X = X,
Z = Z,
K = K,
sigb2 = trueSigb2,
thb = trueThb,
sigma2 = trueSigma2,
theta = trueTheta,
tau2 = trueTau2,
range = c(0, 100))
sigma2 <- trueSigma2
theta <- trueTheta
sigb2 <- trueSigb2
thb <- trueThb
tau2 <- trueTau2
beta <- trueBeta
# Beta
trueBeta <- rep(0, p+1)
##########################
# Generate training data #
set.seed(mySeed)
X <- matrix(runif(n*p, 0, 10), nrow = n, ncol = p)
Z <- matrix(runif(2 * S, 0, 100), ncol = 2)
train <- spatialData(n = n,
X = X,
Z = Z,
K = K,
sigb2 = trueSigb2,
thb = trueThb,
sigma2 = trueSigma2,
theta = trueTheta,
tau2 = trueTau2,
beta = trueBeta,
range = c(0, 100))
sigma2 <- trueSigma2
theta <- trueTheta
sigb2 <- trueSigb2
thb <- trueThb
tau2 <- trueTau2
beta <- trueBeta
range <- c(0, 100)
dims <- 2
tol <- 1e-6
intercept <- TRUE
covariance <- "exponential"
U <- NULL
# Sample the locations and put them into an n-by-dims matrix
# Unless the coordinates are pre-supplied
if (is.null(U)) {
locations <- runif(n * dims, range[1], range[2])
U <- matrix(locations, nrow = n, ncol = dims)
}
# Order U by sum of coordinates
U <- U[order(rowSums(U)), ]
# Compute the covariance matrix (symmetric)
if (covariance == "exponential") {
D <- rdist(U)
} else if (covariance == "exp_squared") {
D <- rdist(U)^2
} else {
stop("Covariance function must be either exponential or exp_squared.")
}
C <- lapply(1:K, function(k) {
sigma2[k] * exp(-theta[k] * D)
})
# Covariance - eta
BF <- Bsplines_2D(Z, df = c(sqrt(K), sqrt(K)))
S <- nrow(Z)
C.array <- simplify2array(C)  # array of dim n x n x K
C.array <- aperm(C.array, c(3, 1, 2))  # now K x n x n
W.list <- lapply(1:K, function(k) tcrossprod(BF[, k]))  # each S x S
W.array <- simplify2array(W.list)  # S x S x K
C.eta <- Reduce('+', lapply(1:K, function(k) kronecker(W.array[, , k], C.array[k, , ])))
# Covariance - beta
n <- nrow(X)
if (intercept == TRUE) {
X0 <- cbind(rep(1, n), X)
} else {
X0 <-  X
}
q <- ncol(X0)
CB <- lapply(1:q, \(j) sigb2[j] * exp(-thb[j] * D))
CXB <- Reduce("+", lapply(1:q, \(j) matrix(X0[ , j], nrow = n, ncol = n) * CB[[j]] *
matrix(X0[ , j], nrow = n, ncol = n, byrow = T)))
lon <- U[ , 1]
lat <- U[ , 2]
beta.list <- vector("list", length = q)
for (j in 1:q) {
beta.list[[j]] <- mvtnorm::rmvnorm(1, sigma = CB[[j]])
}
beta.surf <- Reduce("cbind", beta.list)
dim(beta.surf)
lon <- U[ , 1]
lat <- U[ , 2]
beta.list <- vector("list", length = q)
for (j in 1:q) {
beta.list[[j]] <- t(mvtnorm::rmvnorm(1, sigma = CB[[j]]))
}
beta.surf <- Reduce("cbind", beta.list)
B <- Reduce("cbind", lapply(1:q, \(j) t(rmvnorm(1, sigma = CB[[j]])))) + beta.surf
dim(beta.surf)
source("../other_functions/spatial_data_scen11.R")
source("../other_functions/bsplines_2_3D.R")
mySeed <- 45213
# Sample sizes
# Can have a "small" dataset with n = 100 and nTest = 25
# Then a "large" dataset with n = 500 and nTest = 100
n <- 100
nTest <- 25
# Number of subjects - can probably leave these alone
S <- 10
STest <- 10
# Number of predictors
K <- 9
p <- 2
### True parameter values ###
# Need to play around with these
# Covariance parameters for beta
trueSigb2 <- seq(0.5, 1, length = p + 1)
trueThb <- seq(0.1, 0.2, length = p + 1)
# Covariance parameters for global covariates (each length K)
trueSigma2 <- seq(5, 10, length = K)
trueTheta <- seq(0.1, 0.5, length = K)
# Error variance
trueTau2 <- 0.2
# Beta
trueBeta <- rep(0, p+1)
##########################
# Generate training data #
set.seed(mySeed)
X <- matrix(runif(n*p, 0, 10), nrow = n, ncol = p)
Z <- matrix(runif(2 * S, 0, 100), ncol = 2)
train <- spatialData(n = n,
X = X,
Z = Z,
K = K,
sigb2 = trueSigb2,
thb = trueThb,
sigma2 = trueSigma2,
theta = trueTheta,
tau2 = trueTau2,
beta = trueBeta,
range = c(0, 100))
saveRDS(train, file = "../data/small/scen10/train.RDS")
source("../other_functions/spatial_data_scen11.R")
source("../other_functions/bsplines_2_3D.R")
mySeed <- 45213
# Sample sizes
# Can have a "small" dataset with n = 100 and nTest = 25
# Then a "large" dataset with n = 500 and nTest = 100
n <- 100
nTest <- 25
# Number of subjects - can probably leave these alone
S <- 10
STest <- 10
# Number of predictors
K <- 9
p <- 2
### True parameter values ###
# Need to play around with these
# Covariance parameters for beta
trueSigb2 <- seq(0.5, 1, length = p + 1)
trueThb <- seq(0.1, 0.2, length = p + 1)
# Covariance parameters for global covariates (each length K)
trueSigma2 <- seq(5, 10, length = K)
trueTheta <- seq(0.1, 0.5, length = K)
# Error variance
trueTau2 <- 0.2
# Beta
trueBeta <- rep(0, p+1)
##########################
# Generate training data #
set.seed(mySeed)
X <- matrix(runif(n*p, 0, 10), nrow = n, ncol = p)
Z <- matrix(runif(2 * S, 0, 100), ncol = 2)
train <- spatialData(n = n,
X = X,
Z = Z,
K = K,
sigb2 = trueSigb2,
thb = trueThb,
sigma2 = trueSigma2,
theta = trueTheta,
tau2 = trueTau2,
beta = trueBeta,
range = c(0, 100))
saveRDS(train, file = "../data/small/scen10/train.RDS")
set.seed(mySeed)
indexTest <- sample(n, nTest)
U <- train$U[indexTest, ]
# Generate testing data
set.seed(mySeed)
XTest <- matrix(runif(nTest*p, 0, 10), nrow = nTest, ncol = p)
ZTest <- matrix(runif(2 * STest, 0, 100), ncol = 2)
test <- spatialData(n = nTest,
X = XTest,
Z = ZTest,
U = U,
K = K,
sigb2 = trueSigb2,
thb = trueThb,
sigma2 = trueSigma2,
theta = trueTheta,
tau2 = trueTau2,
beta = trueBeta,
range = c(0, 100))
test$index <- indexTest
saveRDS(test, file = "../data/small/scen10/test.RDS")
summary(train$B)
